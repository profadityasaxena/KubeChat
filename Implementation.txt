# WSL2, Docker, and Kubernetes Setup Guide for Windows

This guide will help you set up a local Kubernetes development environment on Windows using WSL2, Docker Desktop, kubectl, and minikube. It also covers GPU support and running Ollama with GPU acceleration.

---

## Install WSL2 and Ubuntu

Open PowerShell and run:
    wsl --install -d Ubuntu

## Verify WSL Version

Check your WSL version:
    wsl -l -v

If Ubuntu is not using Version 2, set it:
    wsl --set-version Ubuntu 2

## Install Docker Desktop

Download and install Docker Desktop for Windows. During installation, enable the WSL2 backend.

Verify Docker installation:
    docker --version
    docker run hello-world

## Install kubectl

Install kubectl using Windows Package Manager:
    winget install -e --id Kubernetes.kubectl

Verify installation:
    kubectl version --client

## Install minikube

Install minikube:
    winget install -e --id Kubernetes.minikube

## Start a Local Kubernetes Cluster

Start minikube with Docker as the driver:
    minikube start --driver=docker

Verify your cluster:
    kubectl get nodes

---

You now have a local Kubernetes cluster running on Windows with WSL2 and Docker Desktop.

---

## GPU Support in WSL2

Check for NVIDIA GPU support:

Standard check:
    nvidia-smi

If not found, try the WSL path:
    ls -l /usr/lib/wsl/lib/nvidia-smi
    /usr/lib/wsl/lib/nvidia-smi

## Enable Docker-WSL Integration

List Docker contexts:
    docker context ls

You should see a context like `desktop-linux` (the default is marked with *).

## Test GPU Access in Docker

Run a CUDA test container:
    docker run --rm --gpus all nvidia/cuda:12.4.1-base-ubuntu22.04 nvidia-smi

## Run Ollama in Docker with GPU Enabled

Start Ollama:
    docker run --gpus=all -d \
      -p 11434:11434 \
      -v ollama:/root/.ollama \
      --name ollama \
      ollama/ollama

Check the API:
    curl http://localhost:11434/api/tags
    # Should return JSON (possibly empty until you pull a model)

---

## Pull and Test Ollama Models

### a) Download a Small, Fast Model

Pull a lightweight model for quick testing:
    docker exec -it ollama ollama pull llama3.2:1b-instruct

### b) Quick Generation Test via HTTP API

Generate a response using the pulled model:
    curl http://localhost:11434/api/generate \
      -H "Content-Type: application/json" \
      -d '{"model":"llama3.2:1b-instruct","prompt":"Say hello from the GPU."}'

### c) Try Pulling Other Models

Pull a larger model:
    docker exec -it ollama ollama pull llama3:8b-instruct

Or pull a very tiny smoke-test model:
    docker exec -it ollama ollama pull tinyllama

---

## Test GPU Usage During Generation

Open two WSL terminals:

- **Terminal A:** Monitor GPU usage
    watch -n 1 nvidia-smi

- **Terminal B:** Trigger a longer generation
    curl http://localhost:11434/api/generate \
      -H "Content-Type: application/json" \
      -d '{
            "model": "tinyllama",
            "prompt": "Generate 3 short paragraphs explaining how GPUs speed up transformer inference. Mention attention, matrix multiply, and quantization.",
            "options": { "num_predict": 512 }
          }'

---
